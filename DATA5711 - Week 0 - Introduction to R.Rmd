---
title: "DATA5711 Bayesian Computational Statistics"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction to R

**Semester 1, 8 February 2021**

**Objectives:**

* Learn to implement linear regression
* Learn to implement gradient descent
* Learn to solve the coefficients in linear regression model analytically
* Learn how to visualise the dataset

We first load the packages required for this tutorial and read in the data contained in the comma-separated values (csv) file.
```{r, warning=FALSE,message=FALSE}
library(plotly)
library(dplyr)
library(reshape2)
# TO DO
X <- c(0.18,1, 0.92, 0.07, 0.85, 0.99, 0.87)
Y <- c(0.89,0.26,0.11,0.37,0.16,0.41,0.47)
Z <- c(109.85,155.72,137.66, 76.17,139.75,162.6 ,151.77)

x<- array(c(X,Y),dim=c(7,2))
y<- array(c(Z),dim=c(7,1))
print(x)
print(y)


We can look at the two feature sets studied and the corresponding label:
```{r}
X
```

```{r}
Y
```

Let's combine the feature sets and labels into a data frame and visualise them in an (interactive) 3D plot:
```{r}
# TO DO
g<-cbind(x,y)
g

d2 = as.data.frame(g)
d2

colnames(d2)[1]<-"feature1"
colnames(d2)[2]<-"feature2"
colnames(d2)[3]<-"dependent"


install.packages("plot3D")
library("plot3D")

scatter3D(feature1, y, z, ..., colvar = z, col = NULL, add = FALSE)
text3D(x, y, z, labels, colvar = NULL, add = FALSE)
points3D(x, y, z, ...)
lines3D(x, y, z, ...)


```

We can also look at the bivariate plots between each pair of variables:
```{r}
# TO DO

```

# Implementing Linear Regression by Calculating the Coefficients Analytically

The forward model of linear regression with two features takes the following form:
$$y_i = \beta_0 + x_{i1}\beta_1 + x_{i2}\beta_2$$
In matrix form it is:
$$y_i = \left[\begin{array}{ccc} 
1 & x_{i1} & x_{i2}
\end{array}\right] \left[\begin{array}{c} 
\beta_0 \\
\beta_1 \\
\beta_2
\end{array}\right] = \boldsymbol{x}^\top_i\boldsymbol{\beta}$$
which can then be extended to include all observations:
$$
\left[\begin{array}{c} 
y_1 \\
\vdots \\
y_n
\end{array}\right]
=
\left[\begin{array}{ccc} 
1 & x_{11} & x_{12} \\ 
\vdots & \vdots & \vdots \\
1 & x_{n1} & x_{n2}
\end{array}\right]
\left[\begin{array}{c} 
\beta_0 \\ 
\beta_1 \\
\beta_2
\end{array}\right]
$$
$$ \boldsymbol{Y} = \boldsymbol{X\beta}$$
The loss function $J(\beta)$ (also called cost function or objective function) for linear regression is:
$$J(\beta)=|| \boldsymbol{Y} - \boldsymbol{X\beta} ||$$
The coefficients in linear regression can be computed analytically by taking the first derivative and solving analytically:
$$\frac{\partial}{\partial \beta}(\boldsymbol{Y} - \boldsymbol{X\beta})^\top(\boldsymbol{Y} - \boldsymbol{X\beta})=0$$
to give
$$\hat{\beta}=(X^\top X)^{-1} X^\top Y,$$
which, for our dataset, is
```{r}
# TO DO

```

We then add the hyperplane to the 3D plot as follows:
```{r}
# TO DO

```

Next, we make predictions on the testing dataset:
```{r}
# TO DO

```

and add the predictions to the fitted 3D plot:
```{r}
# TO DO

```

# Implementing Linear Regression by Calculating the Coefficients with Gradient Descent

The loss function for linear regression is
$$J(m, c) = \frac{1}{2n} \sum_{i=1}^n (y_i - (m x_i + c))^2$$
Now computing the derivatives of $J(m, c)$ with respect to the gradient and intercept, we have:
$$\frac{\partial}{\partial m}J(m, c)=-\frac{1}{n}\sum_{i=1}^n x_i(y_i - (m x_i + c))$$
$$\frac{\partial}{\partial c}J(m, c)=-\frac{1}{n}\sum_{i=1}^n (y_i - (m x_i + c))$$

We can implement the gradient descent method as follows:
```{r}
# TO DO

```

Using the coefficients from the gradient descent method, we make predictions on the testing dataset:
```{r}
# TO DO

```

We can also add the predictions obtained from the gradient descent method to the previous plot obtained:
```{r}
# TO DO

```